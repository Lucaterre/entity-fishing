Hadoop 2.* config with YARN

// etc/hadoop/hadoop-env.sh 
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_PREFIX=/home/lopez/tools/hadoop/hadoop-2.6.5
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/home/lopez/tools/hadoop/hadoop-2.6.5/etc/hadoop"}

// etc/hadoop/hdfs-site.xml
  <property>
     <name>dfs.replication</name>
     <value>1</value>
  </property>
  
  <property>
     <name>dfs.name.dir</name>
     <value>/home/lopez/tools/hadoop/hadoop-2.6.5/mydata/hdfs/namenode/</value>
  </property>

  <property>
     <name>dfs.data.dir</name>
     <value>/home/lopez/tools/hadoop/hadoop-2.6.5/mydata/hdfs/datanode/</value>
  </property>

// etc/hadoop/yarn-site.xml
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.resource.memory-mb</name>
      <value>13312</value> <!-- total available on the machine 16GB, in pseudo ditributed mode -->
    </property>
    <property>
      <name>yarn.scheduler.minimum-allocation-mb</name>
      <value>1024</value>
    </property>
    <property>
      <name>yarn.scheduler.maximum-allocation-mb</name>
      <value>12288</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>

// etc/hadoop/mapred-site.xml
<configuration>
  <!--property>
    <name>mapred.child.java.opts</name>
    <value>-Xmx1024m</value>
  </property--> <!-- in case we need more memory in the main hadoop job, it should not be the case -->
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.map.memory.mb</name>
    <value>3072</value>
  </property>
  <property>
    <name>mapreduce.reduce.memory.mb</name>
    <value>11264</value> <!-- single reduce job ! -->
  </property>
  <property>
    <name>mapreduce.map.java.opts</name>
    <value>-Xmx3072m</value>
  </property>
  <property>
    <name>mapreduce.reduce.java.opts</name>
    <value>-Xmx10240m</value>
  </property>
  
  <property>
    <name>mapreduce.tasktracker.map.tasks.maximum</name>
    <value>4</value>
  </property>
  <property>
    <name>mapreduce.tasktracker.reduce.tasks.maximum</name>
    <value>1</value>
  </property> <!-- single reduce job ! -->
  <property>
    <name>mapred.reduce.slowstart.completed.maps</name>
    <value>1</value>
  </property>

  <property>
  	<name>mapred.task.timeout</name>
  	<value>1800000</value>
  </property> <!-- timout 30 minutes, safer for building the largest LMDB caches -->
</configuration>

// etc/hadoop/core-site.xml
<configuration>
  <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

Preparing the hdfs space:

~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -mkdir /user
~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -mkdir /user/lopez

~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -put ~/nerd/nerd/data/wikipedia/languages.xml /user/lopez/
~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -put ~/nerd/nerd/data/wikipedia/de-sent.bin /user/lopez/
~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -put ~/nerd/nerd/data/wikipedia/en-sent.bin /user/lopez/

~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -put /mnt/data/wikipedia/latest/enwiki-latest-pages-articles.xml /user/lopez/
~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -put /mnt/data/wikipedia/latest/frwiki-latest-pages-articles.xml /user/lopez/
~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -put /mnt/data/wikipedia/latest/dewiki-latest-pages-articles.xml /user/lopez/

~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -mkdir /user/lopez/output
~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -mkdir /user/lopez/working


Building the hadoop job jar 

...
will create job jar under:
./target/com.scienceminer.nerd-data-0.0.1-job.jar 

launching hadoop process: 

//English
 ~/tools/hadoop/hadoop-2.6.5/bin/hadoop jar com.scienceminer.nerd-data-0.0.1-job.jar /user/lopez/enwiki-latest-pages-articles.xml /user/lopez/languages.xml en /user/lopez/en-sent.bin /user/lopez/working /user/lopez/output

// getting the csv files
 ~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -get /user/lopez/output/* /mnt/data/wikipedia/latest/en/

//French
 ~/tools/hadoop/hadoop-2.6.5/bin/hadoop jar com.scienceminer.nerd-data-0.0.1-job.jar /user/lopez/enwiki-latest-pages-articles.xml /user/lopez/languages.xml fr /user/lopez/en-sent.bin /user/lopez/working /user/lopez/output

// getting the csv files
 ~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -get /user/lopez/output/* /mnt/data/wikipedia/latest/fr/

//German
 ~/tools/hadoop/hadoop-2.6.5/bin/hadoop jar com.scienceminer.nerd-data-0.0.1-job.jar /user/lopez/dewiki-latest-pages-articles.xml /user/lopez/languages.xml de /user/lopez/de-sent.bin /user/lopez/working /user/lopez/output

// getting the csv files
 ~/tools/hadoop/hadoop-2.6.5/bin/hdfs dfs -get /user/lopez/output/* /mnt/data/wikipedia/latest/de/


Expected produced files: there must be 14 generated csv files

articleParents.csv   label.csv       pageLinkOut.csv              stats.csv
categoryParents.csv  page.csv        redirectSourcesByTarget.csv  translations.csv
childArticles.csv    pageLabel.csv   redirectTargetsBySource.csv
childCategories.csv  pageLinkIn.csv  sentenceSplits.csv

Intel Core i7-4790K CPU 4.00GHz Haswell, 16GB memory, with 4 cores, 8 threads, SSD, pseudo distributed mode
French and German Wikipedia XML dump: ~4 hours 
English Wikipedia XML dump: 

Note: right now, work only in pseudo distributed mode (LMDB cache DB are located under local /tmp/) - for cluster level, we need to save the LMDB databases for cache on hdfs and uses distributed haddop caches to access to the cache dbs. 
